\section{\textbf{Related Work}}

\subsection{\textbf{Prompt Engineering and Human Involvement}}
Prompt revision is an essential element of success in engagement with LLMs, since user prompt accuracy and completeness directly affect the model output. The literature indicates increasing recognition that to improve LLM performance, such as in creative writing, data analysis, and educational content creation, it is important to help users develop and enhance prompts.

Recent reviews of the literature establish prompt engineering as a systematic, multidimensional task rather than an ad hoc task. Chen et al. ~\cite{Chen2023Unleashing} have reviewed the current literature by outlining three important dimensions that facilitate effective prompt engineering tasks: task specification, context specification, and formatting the output. This systematic approach to prompt construction provides the theoretical foundation for exploring ways in which different types of scaffolding can help users approach the various aspects of prompt engineering effectively.

The importance of human involvement in prompt optimization is further highlighted by comparative studies of automated versus human-guided approaches. While algorithmic prompt optimization can produce measurable improvements in narrow domains, it frequently fails to capture the nuanced communication styles and context-specific requirements that human users bring to real-world interactions ~\cite{Ramnath2025Systematic}. This limitation underscores the need for assistance conditions that support human-centered prompt revision rather than replacing it entirely.

\subsection{\textbf{Challenges in Prompt Revision}}
Empirical studies consistently demonstrate that users, particularly non-experts, struggle with effective prompt revision even when tasks are well within their capabilities. Research on novice programmers using LLMs for code generation reveals that the primary obstacle is not technical vocabulary but rather the failure to provide sufficient useful information in prompts ~\cite{Lucchetti2024Beginning}. Instead of making substantive improvements, users typically focus on minor stylistic edits, leaving core problems of underspecification unaddressed.

The scope of underspecification challenges extends beyond programming contexts. Yang et al. ~\cite{Yang2025Underspecified} examined underspecified prompts across multiple domains and found that while LLMs can sometimes infer missing details, such prompts are twice as likely to produce suboptimal results when the model or context changes, sometimes causing accuracy drops exceeding 20\%. Significantly, their analysis revealed that simply adding more requirements without strategic consideration can worsen performance, emphasizing the need for guidance tools that help users identify and articulate the most relevant information.

Several frameworks have been proposed to address the challenges identified in prompt revision research. The relationship between user AI literacy and prompt quality provides one lens for understanding how structured support might compensate for expertise gaps. Knoth et al. ~\cite{Knoth2024AI} demonstrate that users with higher AI literacy produce significantly better-performing prompts, suggesting that structured assistance may partially substitute for formal training by embedding best practices directly into the interaction workflow.

\subsection{\textbf{Assistance Strategies and Comparative Studies}}
Template-based approaches draw on this principle by providing explicit scaffolding for prompt construction. Research on real-world prompt templates reveals that practitioners often rely on predefined structures with clear placeholders for context, task descriptions, and constraints ~\cite{Mao2025Prompts}. These structured frameworks help users systematically address the key dimensions of effective prompts while reducing the cognitive load associated with prompt construction. The effectiveness of example-driven approaches is supported by broader research on prompt engineering techniques, where high-quality demonstrations guide model behavior without requiring users to master complex prompting rules ~\cite{Chen2023Unleashing}. This approach aligns with cognitive learning theories that emphasize the value of concrete models in skill acquisition. Feedback-based assistance represents a more dynamic approach to prompt revision support. Research on explainability and context-aware guidance demonstrates that systems providing targeted critique and contextualized suggestions help users better understand prompt-response dynamics and make more effective revisions ~\cite{Mao2025Prompts}. Unlike static templates or examples, feedback-based approaches can adapt to specific user needs and prompt contexts, potentially offering more personalized support.


The iterative nature of effective prompt revision is exemplified in domain-specific studies, such as the Re\textsuperscript{3} system for story generation, where prompts undergo recursive revision and output reranking to maintain coherence with given premises ~\cite{Yang2022Re3}. Again, systematic comparisons of prompting techniques provide additional evidence for the importance of method selection. Saxena et al. ~\cite{Knoth2024AI} compared 14 prompting techniques across 10 software engineering tasks and four LLMs, revealing substantial variation in technique effectiveness across different contexts. Their methodology of systematically varying prompting strategies while holding tasks constant directly informs the within-participant design adopted in the present study.


The literature converges on several key insights directly relevant to domain-specific assistance conditions. First, revision quality depends critically on users' ability to recognize missing or ambiguous information, a challenge documented across studies of novice-LLM interaction  ~\cite{Lucchetti2024Beginning} ~\cite{Yang2025Underspecified} and influenced by factors such as AI literacy ~\cite{Knoth2024AI} and the availability of structured guidance ~\cite{Khurana2024Why}. Second, assistance conditions vary systematically in how they externalize and convey prompting best practices: templates impose upfront structure, examples model successful outcomes for imitation, and feedback provides reactive, targeted critique. Third, measuring the impact of assistance conditions requires both behavioral metrics (revision frequency, completion time) and subjective assessments (perceived effectiveness), as emphasized in frameworks advocating multi-dimensional evaluation of prompting interventions. The movement toward "prompt science" ~\cite{Webson2025From} further legitimizes controlled, hypothesis-driven studies that systematically compare multiple assistance approaches within consistent experimental designs.


However, significant gaps remain in our understanding of how different assistance conditions perform across varied task domains and user populations. Most existing research focuses on single domains or specific user groups, limiting generalizability. Additionally, while individual assistance approaches have been studied in isolation, systematic within-subject comparisons of multiple assistance conditions remain rare. The present study addresses these gaps by examining template-based, example-based, feedback-based, and control conditions across creative writing, data analysis, and educational content generation tasks, providing empirical evidence to guide the development of more effective prompt revision support systems.