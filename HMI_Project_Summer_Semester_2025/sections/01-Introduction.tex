\section{\textbf{Introduction}}

Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable capabilities across diverse domains, from creative writing to data analysis and educational content generation. However, the quality of their outputs is highly dependent on the quality of the prompts they receive. Prompt revision, the iterative process of refining a prompt to better convey intent, plays a critical role in maximizing LLM performance.
\paragraph{}
Despite this importance, there is limited empirical understanding of how users approach prompt revision when supported by structured assistance. Prior studies indicate that beginners often lack the technical vocabulary to articulate precise requirements which leads to under specified prompts, superficial edits, and degraded accuracy in LLM responses. These challenges persist even for tasks within the user's skill level and are compounded by the absence of systematic comparisons between different forms of support. Lucchetti et al.~\cite{Lucchetti2025Substance} found that prompt success depends heavily on the completeness of information provided, with students frequently becoming stuck making minor, stylistic edits rather than adding essential missing details. Similarly, Nguyen et al.~\cite{Nguyen2024Beginning} reported that non-experts face persistent difficulties not only in crafting effective prompts but also in revising them after incorrect outputs, even for tasks designed to match their capabilities.
\paragraph{}
At the same time, advances in LLM architectures introduced intelligent routing, enhanced reasoning, reduced hallucinations, and improved multi-modal capabilities making these systems more powerful than ever. These advances raise the stakes for effective human--LLM interaction and the benefits of more capable models can only be realized when users are able to systematically craft and refine prompts. This highlights the need for guidance methods that not only compensate for users' limited technical vocabulary but also scaffold the revision process in ways that improve usability, effectiveness, and overall task outcomes.
\paragraph{}
These insights reveal a critical research gap. While individual assistance approaches such as template-based, example-based, or feedback-based structured assistance have been studied in isolation~\cite{Khurana2024Why,Mao2025Prompts}, there remains little systematic evidence comparing their relative effectiveness across different task domains. Furthermore, the cognitive processes underlying prompt revision and the ways in which structured support influences these processes are not yet well understood.
\paragraph{Research Question:}
How does the effectiveness of structured assistance for prompt revision vary across different task domains in LLM-assisted tasks?
\paragraph{Contributions:}
This study makes significant contributions to human-computer interaction and prompt engineering research.
\begin{itemize}
    \item We provide quantitative data on four conditions in several task domains, showing evidence of which assistance conditions best support prompt revision.
    \item We propose and validate measures of prompt revision effectiveness, measures of revision frequency, and measures of user confidence, which provide measures for future research.
    \item We developed evidence-based recommendations to design LLM interfaces that would better support us in productive prompting design and revision.
\end{itemize}
