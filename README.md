# Domain-Specific Effectiveness of Structured Assistance for Prompt Revision in LLM-Assisted Tasks

## 📋 Overview

This repository contains the materials and analysis for a research study investigating the effectiveness of different Structured Assistance for Prompt Revision in LLM-Assisted Tasks.

## 🎯 Study Objectives

- Evaluate the effectiveness of different assistance conditions for prompt revision
- Analyze user behavior and feedbacks across various task domains
- Measure time efficiency and revision frequency under different support systems
- Provide insights for improving prompt engineering tools and educational approaches

## 🔧 Methodology

### Assistance Conditions Tested

The study evaluated four distinct assistance methods:

1. **Template-based Assistance**: Users receive structured templates to guide prompt construction
2. **Example-based Assistance**: Users are provided with examples of effective prompts
3. **Feedback-based Assistance**: Users receive iterative feedback on their prompt revisions
4. **Control Group**: No assistance provided (baseline condition)

### Data Collection Process

1. **Survey Preparation**: Survey forms were designed and deployed using SoSci Survey platform
2. **Consent form link**: https://drive.google.com/file/d/1SyBwz22Nuc08cysF3BYIgiEZTESPkSlV/view 
3. **Script Generation**: ChatGPT was utilized to create standardized scripts for each assistance condition
4. **User Participation**: Participants completed tasks under all four assistance conditions
5. **Manual Data Logging**: Time spent and revision frequency were manually recorded for each participant
6. **Participant Recognition**: Certificates were awarded to participants upon study completion

## 📊 Data Analysis

Analysis was conducted using **Google Colab**, focusing on:
- Comparative effectiveness across assistance methods
- Time efficiency metrics
- Revision frequency patterns
- User satisfaction and feedback


## 🚀 Getting Started

### Prerequisites
- Access to Google Colab for running analysis notebooks
- Python 3.7+ (for local analysis)
- Required packages: pandas, matplotlib, seaborn, scipy

### Running the Analysis

1. Clone this repository:
```bash
git clone https://github.com/onirban27/HMI-Project-Summer-2025.git
```

2. Open the analysis notebooks in Google Colab:
   - Navigate to `analysis/notebooks/`
   - Upload the notebooks to your Google Colab environment
   - Follow the step-by-step analysis process

3. Access the data:
   - Raw survey data is available in `data/`

## 📈 Key Findings

- Comparative effectiveness of each assistance condition
- Time efficiency analysis across conditions
- User preference data analysis

## 🔗 Survey Access

The survey form used in this study was hosted on SoSci Survey. For researchers interested in replicating this study, the survey configuration files are available in the (https://www.soscisurvey.de/tutorial490038/) directory.
Video Tutorial: https://drive.google.com/file/d/1aA-O708UHF6NGf7jSXOw5ciSpd4GKwL9/view

## 👥 Participants

- **Total Participants**: 33

## 🏆 Acknowledgments

- Participants who contributed their time and insights to this study
- SoSci Survey platform for hosting the data collection
- ChatGPT for assistance in generating standardized scripts
- Google Colab for providing the analysis environment


## 📞 Contact

For questions about this study or to access additional data, please contact:
- **Researcher**: M Onirban
- **Email**: m.onirban@stud.fra-uas.de
- **Institution**: Frankfurt Applied Sciences

## 📝 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🔄 Contributing

We welcome contributions to improve the analysis or extend the study. Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request with detailed description of changes

---

*This study contributes to the growing body of research on human-AI interaction and prompt engineering effectiveness.*